{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cab-Driver Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "from collections import deque\n",
    "import collections\n",
    "import pickle\n",
    "import time \n",
    "\n",
    "# for building DQN model\n",
    "from keras import layers\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# for plotting graphs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import the environment\n",
    "from Env import CabDriver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining Time Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the time matrix provided\n",
    "Time_matrix = np.load(\"TM.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug print flag \n",
    "\n",
    "DEBUG = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tracking the state-action pairs for checking convergence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialise_tracking_states():\n",
    "#     sample_q_values = [('1-14-1', '2-5'), ('1-14-1', '2-4'), ('1-14-1', '0-1'), ('1-14-1', '4-5'), ('1-14-1', '3-4'), ('1-14-1', '1-2'), ('1-14-1', '2-3')]    \n",
    "    sample_q_values = [ ('1-20-6', '0-1'),\n",
    "                        ('1-20-6', '0-2'),\n",
    "                        ('1-20-6', '0-3'),\n",
    "                        ('1-20-6', '0-4'),\n",
    "                        ('1-20-6', '1-0'),\n",
    "                        ('1-20-6', '1-2'),\n",
    "                        ('1-20-6', '1-3'),\n",
    "                        ('1-20-6', '1-4'),\n",
    "                        ('1-20-6', '4-1'),\n",
    "                        ('1-20-6', '4-2'),\n",
    "                        ('1-20-6', '4-3'),\n",
    "                        ('1-20-6', '0-0'),\n",
    "                        ('1-20-6', '0-1'),\n",
    "                        ('1-20-6', '0-2'),\n",
    "                        ('1-5-5', '1-0'),\n",
    "                        ('1-5-5', '1-2'),\n",
    "                        ('1-5-5', '1-3'),\n",
    "                        ('1-5-5', '1-4'),\n",
    "                        ('1-5-5', '2-0'),\n",
    "                        ('1-5-5', '2-1'),\n",
    "                        ('1-5-5', '2-3'),\n",
    "                        ('1-5-5', '2-4'),\n",
    "                        ('1-5-5', '3-0'),\n",
    "                        ('1-5-5', '3-1'),\n",
    "                        ('1-5-5', '3-2'),\n",
    "                        ('1-5-5', '3-4'),\n",
    "                        ('1-5-5', '4-0'),\n",
    "                        ('1-5-5', '4-1'),\n",
    "                        ('1-5-5', '4-2'),\n",
    "                        ('1-5-5', '4-3')]\n",
    "    for q_value in sample_q_values:\n",
    "        state = q_value[0]\n",
    "        action = q_value[1]\n",
    "        States_track[state][action] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_tracking_states(curr_state, curr_action, q_value):\n",
    "    for state in States_track.keys():\n",
    "        if state == curr_state:\n",
    "            for action in States_track[state].keys():\n",
    "                if action == curr_action:\n",
    "                    States_track[state][action].append(q_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_string(state_or_action):\n",
    "    return ('-'.join(str(e) for e in state_or_action))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining a function to save the Q-dictionary as a pickle file\n",
    "def save_obj(obj, name ):\n",
    "    with open(name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent Class\n",
    "\n",
    "If you are using this framework, you need to fill the following to complete the following code block:\n",
    "1. State and Action Size\n",
    "2. Hyperparameters\n",
    "3. Create a neural-network model in function 'build_model()'\n",
    "4. Define epsilon-greedy strategy in function 'get_action()'\n",
    "5. Complete the function 'append_sample()'. This function appends the recent experience tuple <state, action, reward, new-state> to the memory\n",
    "6. Complete the 'train_model()' function with following logic:\n",
    "   - If the memory size is greater than mini-batch size, you randomly sample experiences from memory as per the mini-batch size and do the following:\n",
    "      - Initialise your input and output batch for training the model\n",
    "      - Calculate the target Q value for each sample: reward + gamma*max(Q(s'a,))\n",
    "      - Get Q(s', a) values from the last trained model\n",
    "      - Update the input batch as your encoded state and output batch as your Q-values\n",
    "      - Then fit your DQN model using the updated input and output batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size, action_space):\n",
    "        # Define size of state and action\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.action_space = action_space\n",
    "\n",
    "        # Write here: Specify you hyper parameters for the DQN\n",
    "        self.discount_factor = 0.99\n",
    "        self.learning_rate = 0.001       \n",
    "        self.epsilon_max = 1.0\n",
    "        self.epsilon_decay = 0.0009 #0.999\n",
    "        self.epsilon_min = 0\n",
    "        \n",
    "        self.batch_size = 32    \n",
    "        # create replay memory using deque\n",
    "        self.memory = deque(maxlen=2000)\n",
    "\n",
    "        # create main model and target model\n",
    "        self.model = self.build_model()\n",
    "\n",
    "    # approximate Q function using Neural Network\n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "        # Write your code here: Add layers to your neural nets\n",
    "        model.add(Dense(32, input_dim=self.state_size, activation='relu',\n",
    "                        kernel_initializer='he_uniform'))\n",
    "        model.add(Dense(32, activation='relu',\n",
    "                        kernel_initializer='he_uniform'))\n",
    "        model.add(Dense(self.action_size, activation='linear',\n",
    "                        kernel_initializer='he_uniform'))\n",
    "        # model.summary() \n",
    "        \n",
    "        model.compile(loss='mse',optimizer=Adam(lr=self.learning_rate))\n",
    "        model.summary\n",
    "        return model\n",
    "\n",
    "    def get_epsilon(self, time):\n",
    "        return self.epsilon_min + (self.epsilon_max - self.epsilon_min) * np.exp(-self.epsilon_decay*time)\n",
    "\n",
    "    def get_action(self, state, time):\n",
    "    # Write your code here:\n",
    "    # get action from model using epsilon-greedy policy\n",
    "    # Decay in ε after we generate each sample from the environment       \n",
    "\n",
    "        epsilon = self.get_epsilon(time)\n",
    "\n",
    "        possible_actions_index, all_possible_actions = env.requests(state)\n",
    "\n",
    "        if len(possible_actions_index) == 0:\n",
    "            return all_possible_actions[0]\n",
    "\n",
    "        if (np.random.rand() <= epsilon):\n",
    "            return random.choice(all_possible_actions)\n",
    "        # if generated random number is greater than ε, choose the action which has max Q-value\n",
    "        else:\n",
    "            state = env.state_encod_arch1(state)\n",
    "            state = state.reshape(1, self.state_size)\n",
    "            q_values = self.model.predict(state)[0]\n",
    "            \n",
    "            argmax = np.argmax(q_values[possible_actions_index])\n",
    "#             DEBUG and print(f'q_values: {q_values}, available_q_vals: {q_values[possible_actions_index]}, argmax: {argmax}, all_possible_actions: {all_possible_actions}')\n",
    "            return all_possible_actions[argmax]\n",
    "\n",
    "    def append_sample(self, state, action, reward, next_state):\n",
    "    # Write your code here:\n",
    "    # save sample <s,a,r,s'> to the replay memory\n",
    "        self.memory.append((state, action, reward, next_state))\n",
    "    \n",
    "    def get_q_values(self, state):\n",
    "        return self.model.predict(state)[0]\n",
    "    \n",
    "    # pick samples randomly from replay memory (with batch_size) and train the network\n",
    "    def train_model(self):\n",
    "        if len(self.memory) > self.batch_size:\n",
    "            # Sample batch from the memory\n",
    "            mini_batch = random.sample(self.memory, self.batch_size)\n",
    "            update_output = np.zeros((self.batch_size, self.state_size)) # write here\n",
    "            update_input = np.zeros((self.batch_size, self.state_size)) # write here\n",
    "            \n",
    "            actions, rewards = [], []\n",
    "            \n",
    "            for i in range(self.batch_size):\n",
    "                state, action, reward, next_state = mini_batch[i]\n",
    "                # Write your code from here\n",
    "                # 1. Predict the target from earlier model\n",
    "                update_input[i] = env.state_encod_arch1(state).reshape(1, self.state_size)\n",
    "                actions.append(action)\n",
    "                rewards.append(reward)\n",
    "                update_output[i] = env.state_encod_arch1(next_state).reshape(1, self.state_size)                \n",
    "                \n",
    "            # 2. Get the target for the Q-network\n",
    "            target = self.model.predict(update_input)\n",
    "            target_qval = self.model.predict(update_output)\n",
    "            \n",
    "            #3. Update your 'update_output' and 'update_input' batch\n",
    "            for i in range(self.batch_size):\n",
    "                # if done[i]:\n",
    "                #     target[i][actions[i]] = rewards[i]\n",
    "                # else: # non-terminal state\n",
    "                action_idx = np.where(action_space == actions[i])\n",
    "                target[i][action_idx] = rewards[i] + self.discount_factor * np.max(target_qval[i])\n",
    "\n",
    "            # 4. Fit your model and track the loss values\n",
    "            self.model.fit(update_input, target, batch_size=self.batch_size, epochs=1, verbose=0)\n",
    "\n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Episodes = 1\n",
    "\n",
    "env = CabDriver()\n",
    "action_space, state_space, state = env.reset()\n",
    "action_size = len(action_space)\n",
    "state_size = len(env.state_encod_arch1(state_space[0]))\n",
    "\n",
    "agent = DQNAgent(state_size, action_size, action_space)\n",
    "\n",
    "States_track = collections.defaultdict(dict)\n",
    "initialise_tracking_states()\n",
    "\n",
    "scores = []\n",
    "\n",
    "start_time = time.time()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action time: 0.0009984970092773438\n",
      "Step time: 0.0\n",
      "Train time: 0.0\n",
      "Action time: 0.0\n",
      "Step time: 0.0\n",
      "Train time: 0.0\n",
      "Action time: 0.0\n",
      "Step time: 0.0\n",
      "Train time: 0.0\n",
      "Action time: 0.0\n",
      "Step time: 0.0\n",
      "Train time: 0.0009961128234863281\n",
      "Action time: 0.0\n",
      "Step time: 0.0\n",
      "Train time: 0.0\n",
      "Action time: 0.0\n",
      "Step time: 0.0\n",
      "Train time: 0.0\n",
      "Action time: 0.0\n",
      "Step time: 0.0\n",
      "Train time: 0.0\n",
      "Action time: 0.0\n",
      "Step time: 0.0\n",
      "Train time: 0.0\n",
      "Action time: 0.0\n",
      "Step time: 0.0\n",
      "Train time: 0.0\n",
      "Action time: 0.0\n",
      "Step time: 0.0\n",
      "Train time: 0.0\n",
      "Action time: 0.0\n",
      "Step time: 0.0\n",
      "Train time: 0.0\n",
      "Action time: 0.0\n",
      "Step time: 0.0\n",
      "Train time: 0.0\n",
      "Action time: 0.0\n",
      "Step time: 0.0\n",
      "Train time: 0.0\n",
      "Action time: 0.0\n",
      "Step time: 0.0\n",
      "Train time: 0.0\n",
      "Action time: 0.0\n",
      "Step time: 0.0\n",
      "Train time: 0.0\n",
      "Action time: 0.0\n",
      "Step time: 0.0\n",
      "Train time: 0.0\n",
      "Action time: 0.0\n",
      "Step time: 0.0\n",
      "Train time: 0.0\n",
      "Action time: 0.0\n",
      "Step time: 0.0\n",
      "Train time: 0.0\n",
      "Action time: 0.0\n",
      "Step time: 0.0\n",
      "Train time: 0.0\n",
      "Action time: 0.0\n",
      "Step time: 0.001001119613647461\n",
      "Train time: 0.0\n",
      "Action time: 0.0\n",
      "Step time: 0.0\n",
      "Train time: 0.0\n",
      "Action time: 0.0\n",
      "Step time: 0.0\n",
      "Train time: 0.0\n",
      "Action time: 0.0009984970092773438\n",
      "Step time: 0.0\n",
      "Train time: 0.0\n",
      "Action time: 0.0009953975677490234\n",
      "Step time: 0.0\n",
      "Train time: 0.0\n",
      "Action time: 0.0\n",
      "Step time: 0.0\n",
      "Train time: 0.0\n",
      "Action time: 0.0\n",
      "Step time: 0.0\n",
      "Train time: 0.0\n",
      "Action time: 0.0\n",
      "Step time: 0.0\n",
      "Train time: 0.0\n",
      "Action time: 0.000997304916381836\n",
      "Step time: 0.0\n",
      "Train time: 0.0\n",
      "Action time: 0.0\n",
      "Step time: 0.0\n",
      "Train time: 0.0\n",
      "Action time: 0.0\n",
      "Step time: 0.0\n",
      "Train time: 0.0\n",
      "Action time: 0.0\n",
      "Step time: 0.0\n",
      "Train time: 0.0\n",
      "Action time: 0.0\n",
      "Step time: 0.0\n",
      "Train time: 0.0\n",
      "Action time: 0.0\n",
      "Step time: 0.0\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x000001CF3E021B88> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x000001CF3E021B88> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001CF3E0D11F8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001CF3E0D11F8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Train time: 1.1829943656921387\n",
      "Action time: 0.0\n",
      "Step time: 0.0\n",
      "Train time: 0.21900367736816406\n",
      "Action time: 0.0009827613830566406\n",
      "Step time: 0.0\n",
      "Train time: 0.22001004219055176\n",
      "Action time: 0.0\n",
      "Step time: 0.0\n",
      "Train time: 0.1999950408935547\n",
      "Action time: 0.0\n",
      "Step time: 0.0\n",
      "Train time: 0.3089926242828369\n",
      "Action time: 0.0\n",
      "Step time: 0.0\n",
      "Train time: 0.30100083351135254\n",
      "Action time: 0.0\n",
      "Step time: 0.0\n",
      "Train time: 0.28999853134155273\n",
      "Action time: 0.0\n",
      "Step time: 0.0\n",
      "Train time: 0.2709965705871582\n",
      "Action time: 0.0\n",
      "Step time: 0.0\n",
      "Train time: 0.29799985885620117\n",
      "Action time: 0.0\n",
      "Step time: 0.0\n",
      "Train time: 0.4379899501800537\n",
      "Action time: 0.0\n",
      "Step time: 0.0\n",
      "Train time: 0.30899834632873535\n",
      "Action time: 0.0\n",
      "Step time: 0.0\n",
      "Train time: 0.24599933624267578\n",
      "Action time: 0.0\n",
      "Step time: 0.0\n",
      "Train time: 0.2370007038116455\n",
      "Action time: 0.0\n",
      "Step time: 0.0\n",
      "Train time: 0.4160020351409912\n",
      "Action time: 0.0\n",
      "Step time: 0.0\n",
      "Train time: 0.2889995574951172\n",
      "Action time: 0.0010006427764892578\n",
      "Step time: 0.0\n",
      "Train time: 0.23399996757507324\n",
      "Action time: 0.0\n",
      "Step time: 0.0\n",
      "Train time: 0.22300338745117188\n",
      "Action time: 0.0\n",
      "Step time: 0.0\n",
      "Train time: 0.2059943675994873\n",
      "Action time: 0.0\n",
      "Step time: 0.0\n",
      "Train time: 0.23500323295593262\n",
      "Action time: 0.0\n",
      "Step time: 0.0\n",
      "Train time: 0.29599857330322266\n",
      "Action time: 0.0\n",
      "Step time: 0.0010006427764892578\n",
      "Train time: 0.3189992904663086\n",
      "Action time: 0.0010001659393310547\n",
      "Step time: 0.0010025501251220703\n",
      "Train time: 0.3060007095336914\n",
      "Action time: 0.0\n",
      "Step time: 0.0\n",
      "Train time: 0.20900297164916992\n",
      "Action time: 0.0\n",
      "Step time: 0.0\n",
      "Train time: 0.356992244720459\n",
      "Action time: 0.0\n",
      "Step time: 0.0\n",
      "Train time: 0.2429978847503662\n",
      "Action time: 0.0\n",
      "Step time: 0.0\n",
      "Train time: 0.29599928855895996\n",
      "Action time: 0.0\n",
      "Step time: 0.0\n",
      "Train time: 0.24699759483337402\n",
      "Action time: 0.0\n",
      "Step time: 0.0\n",
      "Train time: 0.21400117874145508\n",
      "Action time: 0.0\n",
      "Step time: 0.0\n",
      "Train time: 0.2779994010925293\n",
      "Action time: 0.0\n",
      "Step time: 0.0\n",
      "Train time: 0.3309965133666992\n",
      "Action time: 0.0\n",
      "Step time: 0.0\n",
      "Train time: 0.2739980220794678\n",
      "Action time: 0.0\n",
      "Step time: 0.0\n",
      "Train time: 0.3350081443786621\n",
      "Action time: 0.0\n",
      "Step time: 0.0009982585906982422\n",
      "Train time: 0.22900080680847168\n",
      "Action time: 0.0009660720825195312\n",
      "Step time: 0.0\n",
      "Train time: 0.21899843215942383\n",
      "Action time: 0.0\n",
      "Step time: 0.0\n",
      "Train time: 0.21097540855407715\n",
      "Action time: 0.0\n",
      "Step time: 0.0\n",
      "Train time: 0.2849996089935303\n",
      "Action time: 0.0\n",
      "Step time: 0.0\n",
      "Train time: 0.2529945373535156\n",
      "Action time: 0.0\n",
      "Step time: 0.0\n",
      "Train time: 0.2850043773651123\n",
      "Action time: 0.0009953975677490234\n",
      "Step time: 0.0\n",
      "Train time: 0.29900288581848145\n",
      "Action time: 0.0009996891021728516\n",
      "Step time: 0.0\n",
      "Train time: 0.37199878692626953\n",
      "Action time: 0.0\n",
      "Step time: 0.0\n",
      "Train time: 0.39099645614624023\n",
      "Action time: 0.0\n",
      "Step time: 0.0\n",
      "Train time: 0.30899858474731445\n",
      "Action time: 0.0009984970092773438\n",
      "Step time: 0.0\n",
      "Train time: 0.3139967918395996\n",
      "Action time: 0.0\n",
      "Step time: 0.0\n",
      "Train time: 0.2359943389892578\n",
      "Action time: 0.0\n",
      "Step time: 0.0\n",
      "Train time: 0.29600954055786133\n",
      "Action time: 0.0010004043579101562\n",
      "Step time: 0.0\n",
      "Train time: 0.27500247955322266\n",
      "Action time: 0.0009996891021728516\n",
      "Step time: 0.0\n",
      "Train time: 0.24200010299682617\n",
      "Action time: 0.0\n",
      "Step time: 0.0\n",
      "Train time: 0.3239922523498535\n",
      "Action time: 0.0\n",
      "Step time: 0.0\n",
      "Train time: 0.26999783515930176\n",
      "Action time: 0.0\n",
      "Step time: 0.0\n",
      "Train time: 0.23700356483459473\n",
      "Action time: 0.0\n",
      "Step time: 0.0010025501251220703\n",
      "Train time: 0.25199437141418457\n",
      "Action time: 0.0\n",
      "Step time: 0.0\n",
      "Train time: 0.2339942455291748\n",
      "Action time: 0.0\n",
      "Step time: 0.0\n",
      "Train time: 0.25701284408569336\n",
      "Action time: 0.0\n",
      "Step time: 0.0\n",
      "Train time: 0.24100232124328613\n",
      "Action time: 0.0\n",
      "Step time: 0.0\n",
      "Train time: 0.26699185371398926\n",
      "Action time: 0.0009982585906982422\n",
      "Step time: 0.0\n",
      "Train time: 0.2270059585571289\n",
      "Action time: 0.0010051727294921875\n",
      "Step time: 0.0\n",
      "Train time: 0.20999670028686523\n",
      "Action time: 0.0\n",
      "Step time: 0.0\n",
      "Train time: 0.3900008201599121\n",
      "Action time: 0.0\n",
      "Step time: 0.0\n",
      "Train time: 0.22600269317626953\n",
      "Action time: 0.0\n",
      "Step time: 0.0\n",
      "Train time: 0.23000407218933105\n",
      "Action time: 0.0\n",
      "Step time: 0.0\n",
      "Train time: 0.20099592208862305\n",
      "Action time: 0.0\n",
      "Step time: 0.0\n",
      "Train time: 0.3339967727661133\n",
      "Action time: 0.0\n",
      "Step time: 0.0\n",
      "Train time: 0.29999542236328125\n",
      "Action time: 0.0\n",
      "Step time: 0.0\n",
      "Train time: 0.29599976539611816\n",
      "Action time: 0.0\n",
      "Step time: 0.0\n",
      "Train time: 0.3090064525604248\n",
      "Action time: 0.0\n",
      "Step time: 0.0\n",
      "Train time: 0.3360023498535156\n",
      "Action time: 0.0\n",
      "Step time: 0.0\n",
      "Train time: 0.38399744033813477\n",
      "Action time: 0.0010039806365966797\n",
      "Step time: 0.0\n",
      "Train time: 0.3000013828277588\n",
      "Action time: 0.0010089874267578125\n",
      "Step time: 0.0\n",
      "Train time: 0.25000643730163574\n",
      "Action time: 0.0\n",
      "Step time: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train time: 0.3090043067932129\n",
      "Action time: 0.0\n",
      "Step time: 0.0\n",
      "Train time: 0.26799488067626953\n",
      "Action time: 0.0\n",
      "Step time: 0.0\n",
      "Train time: 0.3399975299835205\n",
      "Action time: 0.0\n",
      "Step time: 0.0\n",
      "Train time: 0.27599501609802246\n",
      "Action time: 0.0\n",
      "Step time: 0.0\n",
      "Train time: 0.29799985885620117\n",
      "Action time: 0.0009992122650146484\n",
      "Step time: 0.0\n",
      "Train time: 0.33500123023986816\n",
      "Action time: 0.0\n",
      "Step time: 0.0\n",
      "Train time: 0.41300177574157715\n",
      "Action time: 0.00099945068359375\n",
      "Step time: 0.0\n",
      "Train time: 0.34199976921081543\n",
      "Action time: 0.0\n",
      "Step time: 0.0\n",
      "Train time: 0.24900317192077637\n",
      "Action time: 0.0010025501251220703\n",
      "Step time: 0.0\n",
      "Train time: 0.22399568557739258\n",
      "Action time: 0.0\n",
      "Step time: 0.0\n",
      "Train time: 0.22000551223754883\n",
      "Action time: 0.000995635986328125\n",
      "Step time: 0.0\n",
      "Train time: 0.20799922943115234\n",
      "Action time: 0.0\n",
      "Step time: 0.0\n",
      "Train time: 0.26000356674194336\n",
      "Action time: 0.0\n",
      "Step time: 0.0009887218475341797\n",
      "Train time: 0.32399940490722656\n",
      "Action time: 0.0\n",
      "Step time: 0.0\n",
      "Train time: 0.3080027103424072\n",
      "Action time: 0.0\n",
      "Step time: 0.0\n",
      "Train time: 0.24799871444702148\n",
      "Action time: 0.0010004043579101562\n",
      "Step time: 0.0\n",
      "Train time: 0.22699522972106934\n",
      "Action time: 0.0\n",
      "Step time: 0.0010099411010742188\n",
      "Train time: 0.35098791122436523\n",
      "Action time: 0.0\n",
      "Step time: 0.0\n",
      "Train time: 0.2140038013458252\n",
      "Action time: 0.0\n",
      "Step time: 0.0\n",
      "Train time: 0.33299827575683594\n",
      "Action time: 0.0\n",
      "Step time: 0.0\n",
      "Train time: 0.3339958190917969\n",
      "Action time: 0.0009980201721191406\n",
      "Step time: 0.0\n",
      "Train time: 0.23099732398986816\n",
      "Action time: 0.0\n",
      "Step time: 0.0\n",
      "Train time: 0.20500397682189941\n",
      "Action time: 0.0010018348693847656\n",
      "Step time: 0.0010020732879638672\n",
      "Train time: 0.3239929676055908\n",
      "Action time: 0.0\n",
      "Step time: 0.0\n",
      "Train time: 0.23198890686035156\n",
      "Action time: 0.0\n",
      "Step time: 0.0\n",
      "Train time: 0.33099889755249023\n",
      "Action time: 0.0\n",
      "Step time: 0.0\n",
      "Train time: 0.23300552368164062\n",
      "Action time: 0.0\n",
      "Step time: 0.0\n",
      "Train time: 0.2120048999786377\n",
      "Action time: 0.0\n",
      "Step time: 0.001001596450805664\n",
      "Train time: 0.23200583457946777\n",
      "Action time: 0.0010039806365966797\n",
      "Step time: 0.0\n",
      "Train time: 0.20600056648254395\n",
      "Action time: 0.0\n",
      "Step time: 0.0\n",
      "Train time: 0.2270045280456543\n",
      "Action time: 0.0009963512420654297\n",
      "Step time: 0.0\n",
      "Train time: 0.21600580215454102\n",
      "Action time: 0.0\n",
      "Step time: 0.0\n",
      "Train time: 0.23299741744995117\n",
      "Action time: 0.0\n",
      "Step time: 0.0\n",
      "Train time: 0.20400762557983398\n",
      "Action time: 0.0\n",
      "Step time: 0.0\n",
      "Train time: 0.22599577903747559\n",
      "Action time: 0.0\n",
      "Step time: 0.0\n",
      "Train time: 0.20200276374816895\n",
      "Action time: 0.0\n",
      "Step time: 0.0\n",
      "Train time: 0.23000192642211914\n",
      "Action time: 0.0\n",
      "Step time: 0.0\n",
      "Train time: 0.22300386428833008\n",
      "Action time: 0.0\n",
      "Step time: 0.0\n",
      "Train time: 0.21699118614196777\n",
      "Action time: 0.0\n",
      "Step time: 0.0\n",
      "Train time: 0.23599600791931152\n",
      "Action time: 0.0\n",
      "Step time: 0.0\n",
      "Train time: 0.20400667190551758\n",
      "Action time: 0.0\n",
      "Step time: 0.0\n",
      "Train time: 0.21100115776062012\n",
      "Action time: 0.0\n",
      "Step time: 0.0\n",
      "Train time: 0.22499942779541016\n",
      "Score 0: 87.0\n",
      "Tracking time: 0.13500452041625977\n",
      "Entire Operation took 31.910943746566772 seconds\n"
     ]
    }
   ],
   "source": [
    "for episode in range(Episodes):\n",
    "\n",
    "    # Write code here\n",
    "    # Call the environment\n",
    "    # Call all the initialised variables of the environment\n",
    "    \n",
    "\n",
    "    #Call the DQN agent\n",
    "\n",
    "    day = 0\n",
    "    score = 0\n",
    "    terminal_state = False\n",
    "\n",
    "    # reset the state before new episode\n",
    "    _, _, state = env.reset()\n",
    "    initial_state = state\n",
    "        \n",
    "    while not terminal_state:        \n",
    "        # Write your code here\n",
    "        # 1. Pick epsilon-greedy action from possible actions for the current state\n",
    "        \n",
    "        action_start_time = time.time()\n",
    "        action = agent.get_action(state, episode)\n",
    "        print(\"Action time:\",  time.time() -  action_start_time)\n",
    "        \n",
    "        # 2. Evaluate your reward and next state\n",
    "        step_start_time = time.time()\n",
    "        next_state, reward = env.step(state, action, Time_matrix)\n",
    "        print(\"Step time:\",  time.time() -  step_start_time)\n",
    "        \n",
    "        # 3. Append the experience to the memory\n",
    "        agent.append_sample(state, action, reward, next_state)\n",
    "        \n",
    "        # 4. Train the model by calling function agent.train_model\n",
    "        train_start_time = time.time()\n",
    "        agent.train_model()\n",
    "        print(\"Train time:\",  time.time() -  train_start_time)\n",
    "        \n",
    "        # 5. Keep a track of rewards, Q-values, loss\n",
    "        score += reward\n",
    "        scores.append(score)\n",
    "        \n",
    "        # increase the date if the day is changed\n",
    "        if next_state[2] != state[2]:\n",
    "            day = day + 1\n",
    "        \n",
    "#         DEBUG and print(f'state: {state}, action: {action}, next_state: {next_state}, reward: {reward}, day: {day}')\n",
    "\n",
    "        if day > 30:\n",
    "            terminal_state = True\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "    print(f'Score {episode}: {score}')\n",
    "    \n",
    "        # for tracking\n",
    "    \n",
    "    tracking_start_time = time.time()\n",
    "    state_enc = env.state_encod_arch1(initial_state)\n",
    "    state_enc = np.reshape(state_enc, [1, 36])\n",
    "    q_values = agent.get_q_values(state_enc)\n",
    "    \n",
    "    state_string = to_string(initial_state)\n",
    "    \n",
    "    for index in range(len(env.action_space)):\n",
    "        action_string = to_string(env.action_space[index])\n",
    "        save_tracking_states(state_string, action_string, q_values[index])\n",
    "    \n",
    "    print(\"Tracking time:\",  time.time() -  tracking_start_time) \n",
    "    \n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f'Entire Operation took {elapsed_time} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-5, -5.0, -2.0, -13.0, -9.0, -1.0, 39.0, 43.0, 47.0, 63.0, 58.0, 62.0, 57.0, 56.0, 51.0, 73.0, 71.0, 65.0, 76.0, 84.0, 94.0, 89.0, 101.0, 96.0, 104.0, 99.0, 99.0, 107.0, 101.0, 96.0, 64.0, 74.0, 44.0, 42.0, 37.0, 32.0, 44.0, 72.0, 69.0, 46.0, 58.0, 61.0, 56.0, 49.0, 71.0, 66.0, 61.0, 77.0, 98.0, 102.0, 96.0, 116.0, 136.0, 131.0, 126.0, 121.0, 110.0, 100.0, 57.0, 52.0, 42.0, 28.0, 26.0, 51.0, 62.0, 86.0, 81.0, 80.0, 67.0, 64.0, 59.0, 59.0, 41.0, 41.0, 36.0, 12.0, 30.0, 28.0, 40.0, 50.0, 45.0, 44.0, 39.0, 46.0, 25.0, 20.0, 15.0, 21.0, 16.0, -3.0, -4.0, -9.0, 15.0, 4.0, 12.0, 36.0, 49.0, 46.0, 53.0, 48.0, 29.0, 29.0, 11.0, 35.0, 30.0, 54.0, 66.0, 61.0, 56.0, 54.0, 26.0, 21.0, 16.0, 28.0, 64.0, 64.0, 64.0, 64.0, 76.0, 71.0, 66.0, 61.0, 65.0, 68.0, 63.0, 57.0, 52.0, 47.0, 76.0, 71.0, 85.0, 63.0, 87.0, 76.0, 96.0, 109.0, 81.0, 76.0, 46.0, 46.0, 41.0, 72.0, 78.0, 87.0]\n"
     ]
    }
   ],
   "source": [
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tracking Convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Epsilon-decay sample function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Try building a similar epsilon-decay function for your model.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = np.arange(0,10000)\n",
    "epsilon = []\n",
    "for i in range(0,10000):\n",
    "    # epsilon.append(0 + (1 - 0) * np.exp(-0.0009*i))\n",
    "    epsilon.append(agent.get_epsilon(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(time, epsilon)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
